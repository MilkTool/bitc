/* -*- mode: c++ -*-  */

/** @page MixFixProcessing (Plan for) MixFix Processing
 *
 * BitC accepts mixfix expressions. This note describes some of the
 * implementation issues.
 *
 * @section MixFixWhatIs What is MixFix?
 *
 * A ``classic'' mixfix declaration takes one of the forms:
 *
 * @verbatim
mixfix _+_   5     // infix   (starts and ends with an expression)
mixfix _!    9     // postfix (starts with a expression)
mixfix not_  6     // prefix  (ends with an expression)
mixfix (_)   6     // closed  (starts and ends with a quasi-keyword)
mixfix _[_]  6     // multi-part postfix
@endverbatim
 *
 * A given mixfix specification can have an arbitrary number of
 * expression ``holes.'' Holes may not be adjacent. The mixfix syntax
 * adopted in Agda is typical of the breed.
 *
 * Mixfix declarations have the effect of augmenting the expression
 * grammar. Expressions in the primary grammar have some "primary"
 * cases that are higher precedence than any mixfix production
 * (e.g. "a.b"). When the primary parser processes the mixfix_expr
 * production, it gathers the elements into a linear list consisting
 * of expressions. Some of these really are expression nodes that
 * resulted from primary expression parsing. Most are
 * identifiers. Some of the identifiers turn out to be mixfix
 * quasi-keywords introduced by mixfix declarations. Those will be
 * processed/rearranged by the mixfix processor.
 *
 * BitC augments the Agda-style mixfix syntax in two ways:
 * <ul>
 * <li>
 * Two (or more) quasi-keywords can appear in sequence:
 *
 * @verbatim
mixfix _and@not_   5     // infix
...
cond1 and not cond2
@endverbatim
 *
 * </li>
 * <li>
 *
 * A expression parameter can be specified as ``thunked'', providing
 * lazy evaluation:
 *
 * @verbatim
mixfix _and#_   5     // infix
...
cond1 and cond2
... gives ...
_and#_(cond1, lambda () cond2 )
@endverbatim
 *
 * with the consequence that many of the traditional ``syntactic
 * forms'' can be expressed as mixfix rewrites.
 * </li>
 * </ul>
 *
 * The BitC mixfix design borrows two elements from Agda: (1) the
 * ability to specify mixfix rather than just infix operators, and (2)
 * the notion that "interior" (fully bracketed) expressions can be of
 * any precedence. That is: precedence relations are only checked at
 * edge holes. In contrast to Agda, we retain the tradition that
 * precedence relations are transitive. While we treat quasi-keywords
 * as keywords while they are in scope (meaning mainly that they
 * cannot be rebound), we do <em>not</em> require that they be
 * unique. In particular, it is perfectly okay in BitC to introduce
 * the mixfix rules:
 *
 * @verbatim
mixfix while_do#_  5
mixfix until_do#_  5
@endverbatim
 *
 * even though they both use "do" as a quasi-keyword. The fact that
 * quasi-keywords are not unique somewhat increases the challenge of
 * the BitC parsing problem in comparison to other mixfix
 * implementations.
 *
 * @note Actually, this example isn't quite legal because
 * <code>while</code>, <code>until</code>,  and <code>do</code> are
 * keywords, but with the introduction of the mixfix mechanism and the
 * (forthcoming) support for automatic thunking these may move out of
 * the core and get specified as mixfix entries in the prelude.
 *
 * @section MixFixPrecedence Handling of Precedence
 *
 * One of the back-and-forth discussions in the literature is whether
 * precedence relations in mixfix should be transitive. For example,
 * if we know <code>_AND_ &lt; _==_</code> and <code>_==_ &lt;
 * _+_</code>, does it follow that <code>_AND_ &lt; _+_</code> (which
 * is arguably nonsense)? Danielsson argues that transitive precedence
 * relations are undesirable, because they can lead to this sort of
 * meaningless outcome. My view is that uses of mixfix tend to fall
 * into two cases:
 *
 * - Most uses of mixfix "fit" the regular precedence hierarchy
 *   comfortably. That is: you can find an available spot for them in
 *   the available precedence hierarchy and applying the precedence
 *   rules in a transitive way doesn't result in any surprises.
 * - A few uses of mixfix are sophisticated, and are introducing
 *   completely different syntactic schemas than the ones that are
 *   conventional in programming languages. Either there aren't
 *   enough levels available to insert the right operators in the
 *   desired ways, or it is truly necessary to change the
 *   <em>relative</em> precedences of certain operators in the context
 *   of the new sub-language, or perhaps it is desired to
 *   <em>remove</em> some operators that are part of the set provided
 *   by the prelude. That is: you are designing a new, specialized
 *   expression language rather than tweaking an existing one.
 *
 * My (unsubstantiated) belief is that the perceived problems with
 * transitive precedence relations arise mainly in the second
 * situation, and are a result of the fact that existing
 * implementations don't provide a first-class notion of syntax
 * rules. That's easily remedied without too much complexity, so we're
 * going to give it a try in BitC.
 *
 * @section MixFixTheoryOfOperation Theory of Operation
 *
 * MixFix itself is conceptually straightforward, but not documented
 * in a way that is particularly accessible to first-time implementors
 * (like myself). The ``main'' parser (e.g. one built in bison) has
 * expression productions that construct a linear list of expressions,
 * some of which are quasi-keywords. For example, the mixfix rule:
 *
 * @verbatim
mixfix _[_]  6     // postfix with multiple expressions
@endverbatim
 * 
 * introduces the quasi-keywords '[' and ']'. While this rule is in
 * scope, these identifiers will be treated as keywords. The rule
 * also introduces what amounts to a grammar production
 *
 * @verbatim
mixfix_expr: mixfix_expr '[' mixfix_expr ']'  %prec 6
@endverbatim
 *
 * The job of the mixfix layer is to implement these productions. The
 * implementation challenge derives from the fact that the set of
 * productions is not fixed: mixfix declarations can (eventually)
 * occur in local scopes. In the classic table-driven parsing
 * techniques, <em>adding</em> a production is (barely) tractable, but
 * <em>deleting</em> one is pretty darned expensive. So, for example,
 * the AGDA implementation uses memoizing parser combinators, and (I
 * assume - haven't checked) flushes the memoization cache when a
 * production is removed.
 *
 * A mixfix grammar is qualitatively simpler than a CFG:
 *
 * - Two "holes" are never adjacent.
 * - No eta-productions (rules matching empty) are present.
 * - Ambiguities are rare in practice. The "right" production, when
 *   one is present, can generally be determined by looking at the
 *   next two items in the input stream. This typically reduces the
 *   number of productions that must be considered down to one.
 * - Most productions are infix, and selection options on these are
 *   bounded by precedence rules. Two element lookahead (which is
 *   possible because of the "no two holes" rule is often enough to
 *   further filter the candidate rules.
 * - For human reasons, expressions don't tend to get too large before
 *   parenthesis start to appear. This has the effect of bounding N.
 *
 * The implementation chosen here here is a variant on a back-tracking
 * left-corner parser. We first seek a highest-precedence reduction in
 * the left corner, use recursive descent in internal expressions, and
 * then do a precedence-constrained recursive descent parse on the
 * right corner. Back-tracking can occur in internal positions, and in
 * the right-most position when the right-side construction proves to
 * be right-associative. The parser itself is interpreted in order to
 * avoid the challenges of inserting and removing productions from a
 * table-driven parser.
 *
 * Further optimizations are possible to control the cost of
 * back-tracking if this becomes necessary. In particular, observe
 * that if a given input position corresponds to the beginning of an
 * expression, the position may or may not ultimately be the start of
 * an expression, but if it <em>is</em> the start of an expression,
 * the input from that point will always parse the same way. This is
 * true because a mixfix parser only has one syntactic category
 * (i.e. non-terminal). It means that parse sub-trees constructed in
 * previous passes can be retained in pack-rat style.
 *
 * In the present parser, I haven't implemented the pack-rat
 * optimization for several reasons:
 * # right-associative expressions are very rare
 * # some amount of syntactic category structure is imposed in the
 *   bison parser, with the effect that most mixfix parses occur over
 *   very simple trees.
 *
 * @subsection MixFixParseDataStructures Supporting Data Structures
 *
 * The implementation maintains some supporting data structures:
 *
 * - @b MixRule, which records for each rule its name, the RHS elements
 *   of the production, its precedence, its fixity, and its
 *   associativity. The length of the RHS vector matches the "reduce
 *   size" of the production (the number of elements on the parse
 *   stack to remove when reducing).
 *
 * - @b QuasiKeyword, a mapping from strings (identifiers) to a
 *   structure of counters. This structure records:
 *   - How many times the quasi-keyword is in use in some production
 *   - How many times it appears as the first element of a rule
 *   - How many times it appears in some rule just before a trailing hole
 *   - How many times it appears in some rule just after a leading hole
 *   Counters are used instead of bits in order to facilitiate rule
 *   deletion.
 *
 * - @b MixContext, a structure that records a mixfix parsing context
 *   and its associated keyword map.
 *
 * The mixfix parser implementation itself is a recursive, predictive,
 * shunting-yard variant. It uses a distinct parse stack and copy of
 * the input for each production it attempts so that it is easier to
 * back-track.
 *
 * There are several aspects of the BitC mixfix specification that
 * simplify or help to optimizer the parsing problem:
 *
 *   - It is never the case that two expression "holes" are adjacent.
 *   - Every mixfix specification has at least one hole.
 *   - There are no eta-productions to consider.
 *   - Every production either starts with a quasi-keyword (an
 *     operator) or an expression.
 *   - There is only one non-terminal: @em mixfix-expr.
 *   - In any place where there is ambiguity of action, either there
 *     is a disambiguating precedence rule or there is an error.
 *
 * @subsection MixFixComplications Complications
 *
 * After getting an initial mixfix implementation working, I stumbled
 * into an unpleasant surprise. The expression:
 *
 * @verbatim
a + (b-1)
@endverbatim
 *
 * got completely mangled, because function application has higher
 * precedence than anything else. In a mixfix parser, the ``+'' is
 * just an idenfitier, so the <code>+(b-1)</code> got processed as a
 * procedure application in the primary parser before processing ever
 * made it into the mixfix layer. The resulting input at the mixfix
 * layer was:
 *
 * @verbatim
a {apply + {apply - b 1}}
@endverbatim
 *
 * Needless to say, this was not quite what I had in mind. The
 * question is: since ``+'' is just an identifier, why are the
 * following two things handled differently?
 *
 * @verbatim
a + (b-1)
a f (b-1)   // invalid
@endverbatim
 *
 * The answer is that they are different because, at the mixfix layer,
 * the ``+'' is not an identifier; it is a quasi-keyword. In
 * consequence, the input pattern <code>+(b-1)</code> does not have an
 * expression on the left, and therefore will not match the rule for
 * application. "Hang on," you say, "the rule for application?"
 *
 * Yes. That's what this whole ``complications'' discussion is leading
 * up to. We can't make decisions about what is an application until
 * we get into the mixfix layer, which leads us to the following chain
 * of conclusions:
 *
 * - The mixfix layer has to take over the handling of application, so
 *   we need a rule for _(_). 
 * - Since BitC procedures are n-ary, this means that the mixfix layer
 *   also has to take over handling of _,_ in order to handle
 *   arguments, but...
 * - Once mixfix handles _,_ it also needs to handle pair
 *   construction, some of the list convenience syntax, and vector
 *   initializers.
 * - Since those "take over" array indexing, mixfix needs to process
 *   that too. 
 * - Of these, pair construction is a bit unpleasant, because it means
 *   that the (_) rule might mean either ordinary parenthesization or
 *   the pair construction convenience syntax!
 * - And just for fun, mixfix also has to handle _._, because after it
 *   takes over parenthesis we still need to be able to process
 *   <code>(a).b</code>.
 *
 * Here is how I ended up sorting this all out:
 *
 * First, remember that the mixfix layer is building ASTs, and that it
 * is purely context free. Because of this, my description above of
 * the apply pattern doesn't work, and here's what we need to do to
 * sort out pairs and application:
 *
 * - Initially, we treat <code>_,_</code> as building apply ASTs of the
 *   form <code>{apply _,_ e1 e2}</code>
 * - Similarly, we treat <code>(_)</code> as building apply ASTs of the
 *   form {apply <code>(_)</code> e1}
 * - We handle procedure application with the special-case pattern
 *   ``<code>_&nbsp;_</code>'', 
 *   where the second expression is required to be of the form <code>{apply
 *   (_) e1}</code>. So a procedure call with show up as
 *   <code>{ apply __ { apply (_) e1 }}</code> where the <em>e1</em>
 *   will be an 
 *   apply of <code>_,_</code> exactly if the procedure takes multiple arguments.
 * - We handle array indexing similarly to procedure calls using 
 *   ``<code>_&nbsp;_</code>'', 
 *   where the second expression is required to be of the form <code>{apply
 *   [_] e1}</code>.
 * - For list literals <code>[ a, b, c ]</code>, we end up with:
 *   <code>{ apply (_) { apply [_] e1} }</code>
 * - For pair literals <code>(a, b, c)</code>, we end up with:
 *   <code>{ apply (_) { apply _,_ e1 e2} }</code>
 * - For vector literals, we may end up with:
 *   <code>{ apply MakeVector { apply _,_ e1 e2} }</code> or 
 *   <code>{ apply MakeVector (singleton expr) }</code> or 
 *
 * The first important point in all of this is to notice that an
 * <code>{apply _,_
 * e}</code> is <em>always</em> ``wrapped'' by something else in
 * well-formed input, and conversely, that an <code>{apply (_)
 * e}</code> node which is merely a parenthesized expression can be
 * identified by virtue of <em>not</em> being wrapped. 
 *
 * The second important point is that the operator precedence rules
 * for <code>_,_</code>, <code>(_)</code>, and ``<code>_&nbsp;_</code>''
 * (and friends) work out correctly, primarily because
 * <code>_,_</code> always appears inside a closed form.
 *
 * The end result is that we can let the mixfix layer assemble an
 * expression tree by running the precedence-based parser, and then
 * make a post-pass to rewrite that tree into the proper form for
 * literal constructions, vector constructions, and applications.
 *
 * Which, after seeing the initial mess created by:
 *
 * @verbatim
a + (b-1)
@endverbatim
 *
 * is a feasibility conclusion that I was <em>very</em> relieved to reach!
 *
 * @subsection MixFixImplIssues Issues on Implementation
 *
 * When I went to actually implement, I found that the preferred parse
 * rules for handling <code>_,_</code> are not possible. Ideally, the
 * primary parser would ensure that <code>_,_</code> only arises
 * within the legal bracketing forms <code>(_)</code> or
 * <code>[_]</code>. In practice, this causes a long list of parse
 * conflicts. The workaround is to simply accept ``,'' as if it were
 * an identifier, and leave the syntactic issue to be diagnosed in the
 * mixfix handler through the absence of any <code>,_</code> or
 * <code>_,</code> productions.
 *
 * @subsection MixFixParser The Parser
 *
 * The parser a performs a mix of bottom-up and top-down parsing. It
 * is bottom-up on the left and top-down on the right. While
 * attempting to match a rule, a conventional recursive-descent parser
 * can end up testing both left-side and right-side rules
 * repeatedly. The intuition here is that on the left-hand side of the
 * input we are necessarily going to end up reducing the
 * highest-precedence production that matches the leading input, so we
 * might as well reduce that eagerly without redundant effort. That is
 * the bottom-up part of the strategy.
 *
 * But in order to do that, we need to make sure that any hole
 * appearing on the right of the candidate production is matched. This
 * means that we need to do a recursive-descent style parse when we
 * encounter a right-side hole. The interesting case there is when we
 * have a "left rule" of the form ...left_ and a "right rule" of the
 * form _right.... Because of precedence and associativity, these
 * might combine in three ways:
 *
 * @verbatim
left_
    \         right rule has higher precedence,
    _right    or right-associative

    _right    left rule has higher precedence,
    /         or left-associative
left_

left_ <=> _right  non-associative; no parse
@endverbatim
 *
 * Only the first case allows us to combine the two productions into a
 * single case. What we do is a precedence-constrained right
 * recursion. In the other two cases, we simply shift the intervening
 * hole, reduce, push the resulting parse sub-tree back onto the
 * input, and proceed again to attempt a left-corner reduction.
 */
